{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Abdulrahman Alabrash \n",
    "\n",
    "https://github.com/alabrashJr/DCNN-Julia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from saved file \n",
    "load(\"TREC_sib.jld2\",\"datas\")-> returns revs, W, W2, word_idx_map, vocab\n",
    "* revs Dict{String,Any} with 5 entries:y,num_words,tree,text,split\n",
    "    * y-> label of the questions 1-5 \n",
    "    * num_words-> length of questions \n",
    "    * tree -> 46-element Array{Array,1}, each array contains data_sibling(5)+data_tree(5) =10\n",
    "    * text -> the question text \n",
    "    * split -> type of tuple (training, test , div)\n",
    "\n",
    "\n",
    "\n",
    "* W wordembedding generated by google2vec, size(10098×300)\n",
    "* W2 word embedding generated by -0.25 , 0.25 size(10098×300)\n",
    "* word_idx_map Dict{Any,Any} with 10097 entries , unique word id dictionary \n",
    "* vocab DefaultDict{Any,Any,Int64} with 10097 entries, vocab and reptation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg;Pkg.update(); for p in (\"Embeddings\",\"DataFrames\",\"DataStructures\",\"DataFrames\",\"FileIO\",\"LinearAlgebra\",\"Knet\",\"FileIO\"); haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using DataStructures,DataFrames,FileIO,Embeddings,LinearAlgebra,DataFrames;\n",
    "using Base.Iterators: flatten\n",
    "using Statistics: mean\n",
    "using Knet: Knet, conv4, pool, mat, KnetArray, nll, zeroone, progress, sgd, param, param0, dropout, relu, Data,minibatch;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"abbreviation\", \"entity\", \"description\", \"location\" ,\"numeric\",\" \"]\n",
    "revs, W, W2, word_idx_map, vocab=load(\"Data/TREC_sib.jld2\",\"datas\"); \n",
    "word_idx_map[\"ROOT\"]=size(W,1);\n",
    "W=W';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change W2 with prof's method \n",
    "# struct Embed; w; end\n",
    "# Embed(vocabsize::Int,embedsize::Int) = Embed(param(embedsize,vocabsize))\n",
    "# (e::Embed)(x) = e.w[:,x]\n",
    "# W2=Embed(300,length(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_text_mat (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transforms sentence into a list of indices. Pad with zeroes.\n",
    "function get_text_mat(t,word_idx_map;max_l=56,filter_h=5)\n",
    "    #t the text of question\n",
    "    x=[] # output matrix\n",
    "    pad=filter_h -1 # padding number\n",
    "    for i in collect(1:pad);push!(x,0);end #adding padding \n",
    "    words=split(t)\n",
    "    #extract the unique id of words in the question text and adding it to the matrix \n",
    "    for w in words\n",
    "        if w in keys(word_idx_map);push!(x,word_idx_map[w])\n",
    "        else; @show w ;end\n",
    "    end    \n",
    "\n",
    "    while length(x)<max_l+2*pad    # accomplish 64 +1 size by adding zeros till finish \n",
    "            push!(x,0)\n",
    "    end\n",
    "    \n",
    "    return  x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getSen (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function getSen(vector)\n",
    "#labels=[\"abbreviation\",\"numeric\",  \"description\", \"human\",\"location\" ,\"entity\"]\n",
    "t=Array{Int}(vector)\n",
    "println(permutedims(t))\n",
    "for i in t\n",
    "    if i==1557;print(\"?\\n y =\",t[end]+1 );break;end\n",
    "    if i==0;continue;end\n",
    "    for (key,value) in word_idx_map\n",
    "        if value==i; print(key,\" \");end\n",
    "    end\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_tree_rep (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transforms sentence into a list of indices. Pad with zeroes. \n",
    "function get_tree_rep(r,word_idx_map)\n",
    "# question \n",
    "#@show t=r[\"tree\"] #the tree of question\n",
    "    each_sent=deepcopy(r)# output matrix\n",
    "    for (j, each_word) in enumerate(each_sent[1:end-1])\n",
    "        #@show (j, each_word)\n",
    "            for (l, each_field) in enumerate(each_word)\n",
    "           # @show (l, each_field)\n",
    "                if each_field in keys( word_idx_map)\n",
    "                #@show j,l ;\n",
    "                    each_sent[j]=Array{Any,1}(each_sent[j])\n",
    "                     each_sent[j][l] = word_idx_map[each_field]\n",
    "                elseif each_field == 0\n",
    "                    continue\n",
    "                else\n",
    "                    @show each_field\n",
    "                end\n",
    "            end\n",
    "    end       \n",
    "    return each_sent;\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran = 314\n",
      "What was Einstein 's IQ ?\t6\n",
      "Any[0 0 0 0 6965 1624 6086 8251 2181 1557 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "What=6965\n",
      "was=1624\n",
      "Einstein=6086\n",
      "'s=8251\n",
      "IQ=2181\n",
      "?=1557\n"
     ]
    }
   ],
   "source": [
    "ran=rand(1:length(revs));@show ran\n",
    "print(revs[ran][\"text\"]); println(\"\\t\",length(split(revs[ran][\"text\"])))\n",
    "println(permutedims(get_text_mat(revs[ran][\"text\"],word_idx_map;max_l=56) ))\n",
    "for i in split(revs[ran][\"text\"]) ;println(i,\"=\",word_idx_map[i]) ;end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any[\"ROOT\", \"ROOT\", \"ROOT\", \"'s\", \"Einstein\", \"'s\", \"Einstein\", \"*STOP*\", \"*STOP*\", \"IQ\"]\n",
      "Any[10098, 10098, 10098, 8251, 6086, 8251, 6086, 4856, 4856, 2181]\n",
      "ROOT=10098\n",
      "ROOT=10098\n",
      "ROOT=10098\n",
      "'s=8251\n",
      "Einstein=6086\n",
      "'s=8251\n",
      "Einstein=6086\n",
      "*STOP*=4856\n",
      "*STOP*=4856\n",
      "IQ=2181\n"
     ]
    }
   ],
   "source": [
    "tree_ind=1\n",
    "println(revs[ran][\"tree\"][tree_ind])\n",
    "println(permutedims(get_tree_rep(revs[ran][\"tree\"],word_idx_map))[tree_ind])\n",
    "for i in revs[ran][\"tree\"][tree_ind] ;println(i,\"=\",word_idx_map[i]) ;end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×450 Array{Any,2}:\n",
       " \"ROOT\"  \"ROOT\"  \"ROOT\"  \"did\"  \"develop\"  …  \"*ZERO*\"  \"*ZERO*\"  \"*ZERO*\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutedims(vcat(revs[1][\"tree\"][1:end-1]...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_dev_test(revs)\n",
    "    s1,s2,s3=[],[],[]\n",
    "    t1,t2,t3=[],[],[]\n",
    "    for rev in revs\n",
    "    sent =get_text_mat(rev[\"text\"], word_idx_map)   \n",
    "    push!(sent,rev[\"y\"])\n",
    "    sent_tensor = get_tree_rep(rev[\"tree\"], word_idx_map)\n",
    "        \n",
    "    if rev[\"split\"]==1\n",
    "            push!(s1,Array{Int}(sent))\n",
    "            push!(t1,sent_tensor)\n",
    "    elseif rev[\"split\"]==2\n",
    "            push!(s2,Array{Int}(sent))\n",
    "            push!(t2,sent_tensor)\n",
    "    elseif rev[\"split\"]==3\n",
    "            push!(s3,Array{Int}(sent))\n",
    "            push!(t3,sent_tensor)\n",
    "    end\n",
    "end\n",
    "\n",
    "    train = hcat([f1 for f1 in s1]...)\n",
    "    test =hcat([f1 for f1 in s2]...)\n",
    "    dev = hcat([f1 for f1 in s3]...)\n",
    "    train_tensor = t1\n",
    "    test_tensor = t2\n",
    "    dev_tensor = t3\n",
    "    return (train,test,dev),(train_tensor,test_tensor,dev_tensor)\n",
    "end\n",
    "dataset,datasetTensor=train_dev_test(revs);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 276 1189 9009 2836 4868 9231 9009 6820 4226 8045 7196 129 9003 1557 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      "Who is the richest person in the world , without owning a business ?\n",
      " y =4"
     ]
    }
   ],
   "source": [
    "#try on some vectors \n",
    "ran=rand(1:size(dataset[1],2))\n",
    "getSen(dataset[1][:,ran])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset[1] --> train 65×5452 } text data\n",
    "dataset[2]----> test 65×500  }\n",
    "dataset[3] ----> dev 0x0     }\n",
    "\n",
    "datasetTensor[1] ---> train_tensor 5452(46(10)))\n",
    "datasetTensor[2] ---> test_tensor 500(46(10)))\n",
    "datasetTensor[3] --->dev_tensor 0(0(0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "W=Knet.Param(KnetArray{Float32}(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46-element Array{Any,1}:\n",
       " Any[10098, 10098, 10098, 1557, 1679, 1557, 1679, 4856, 4856, 10098] \n",
       " Any[10098, 10098, 1557, 1679, 10098, 1679, 10098, 4856, 4856, 10098]\n",
       " Any[10098, 1557, 1679, 10098, 10098, 627, 1679, 8148, 8148, 10098]  \n",
       " Any[1557, 1679, 10098, 10098, 10098, 1189, 1679, 627, 8148, 10098]  \n",
       " Any[1557, 1679, 10098, 10098, 10098, 9231, 1679, 1557, 4856, 10098] \n",
       " Any[1679, 10098, 10098, 10098, 10098, 6797, 9231, 4856, 4856, 1679] \n",
       " Any[627, 1679, 10098, 10098, 10098, 6965, 627, 8148, 8148, 1679]    \n",
       " Any[1189, 1679, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]  \n",
       " Any[9231, 1679, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]  \n",
       " Any[6797, 9231, 1679, 10098, 10098, 7118, 7118, 7118, 7118, 7118]   \n",
       " Any[6965, 627, 1679, 10098, 10098, 7118, 7118, 7118, 7118, 7118]    \n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " ⋮                                                                   \n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " Any[10098, 10098, 10098, 10098, 10098, 7118, 7118, 7118, 7118, 7118]\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]                                      "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetTensor[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5451,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1=vcat([permutedims(vcat(datasetTensor[1][x][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]);#sent1=vcat([permutedims(vcat(datasetTensor[1][1][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]...)\n",
    "sent2=vcat([permutedims(vcat(datasetTensor[2][x][1:end-1]...)) for x in 1:size(datasetTensor[2],1)]);#sent1=vcat([permutedims(vcat(datasetTensor[1][1][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]...);\n",
    "#sent1=vcat([permutedims(vcat(datasetTensor[1][1][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]...)'\n",
    "size(sent1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a=reshape([1:4]...,(1,1,2,2))\n",
    "b=reshape([5:8]...,(1,1,2,2))\n",
    "cat([a,b]...,dims=4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=Array{Int8}([dataset[1][:,x][end] for x in 1:size(dataset[1],2)]);#ytrainT=[datasetTensor[1][x,:][end][end][end] for x in 1:size(datasetTensor[1],1)];\n",
    "y_test=Array{Int8}([dataset[2][:,x][end] for x in 1:size(dataset[2],2)]);#ytestT=[datasetTensor[2][x,:][end][end][end] for x in 1:size(datasetTensor[2],1)];\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sent1=reshape(sent1,(size(sent1)[1],1,1,1))\n",
    "W=reshape(W,(size(W)[1],size(W)[2],1,1))\n",
    "y_train=reshape(y_train,(size(y_train)[1],1,1,1));\n",
    "#for i in 1:bs:size(sent1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tt=repeat(sent1,160)\n",
    "size(tt)\n",
    "size(reshape(tt,(5451, 1, 1, 160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5451,), (5451,), (300, 10098))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(sent1),size(y_train),size(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"160-element Array{Array{Any,2},1}\", \"160-element Array{Int8,1}\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn=minibatch(sent1,y_train,160,shuffle=true);\n",
    "dtst=minibatch(sent2,y_test,160);\n",
    "#summary.(Iterators.first(dtrn))\n",
    "# (x,y) = first(dtrn)\n",
    "# println.(summary.(x));\n",
    "summary.(Iterators.first(dtrn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a convolutional layer:\n",
    "struct Conv; w; b; f; p;E; end\n",
    "function (c::Conv)(x) \n",
    "#     println(\"\\nConvdeyim \\t\", typeof(x),\"\\t\", summary(x) )\n",
    "    #xx=KnetArray{Float32}(reshape(c.E[:, permutedims(hcat(x...))],(300,450,1,160)))\n",
    "    return c.f.(pool(conv4(c.w, dropout(x,c.p)) .+ c.b))\n",
    "end\n",
    "Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0,E=W) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine dense layer (See mlp.ipynb):\n",
    "struct Dense; w; b; f; p; end\n",
    "function (d::Dense)(x) \n",
    "#     println(\"\\nDensedeyim ,\\t \" , typeof(x),\"\\t\", summary(x))\n",
    "    d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul\n",
    "end\n",
    "Dense(i::Int,o::Int,f=relu;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a chain of layers\n",
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "function (c::Chain)(x) \n",
    "    x=KnetArray{Float32}(reshape(W[:, permutedims(hcat(x...))],(300,450,1,160)))\n",
    "#     println(\"\\nChaindeyim ,\\t \", typeof(x),\"\\t\", summary(x))\n",
    "    (for l in c.layers; x = l(x); end; x)\n",
    "end\n",
    "function (c::Chain)(x,y) \n",
    "#     println(\"\\nloss Chaindeyim x ,\\t \", typeof(x),\"\\t\", summary(x))\n",
    "#     println(\"\\nloss Chaindeyim y ,\\t \", typeof(y),\"\\t\", summary(y))\n",
    "    nll(c(x),y)\n",
    "    \n",
    "end\n",
    "(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden_units=[100,2] #which meaning ...... how many conv layers \n",
    "# dropout_rate=[0.5] #where in which layer ? conv or dense\n",
    "n_epochs=20;\n",
    "# batch_size=170, \n",
    "lr_decay = 0.95\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)\n",
    "        r = ((model(dtrn), model(dtst), zeroone(model,dtrn), zeroone(model,dtst))\n",
    "             for x in takeevery(length(dtrn), progress(sgd(model,repeat(dtrn,n_epochs),lr=lr_decay))))\n",
    "        r = reshape(collect(Float32,flatten(r)),(4,:))\n",
    "        Knet.save(file,\"results\",r)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        if isfile(file);r=Knet.load(file,\"results\");else;println(\"there is no file such this\");return;end\n",
    "    end\n",
    "    println(minimum(r,dims=2))\n",
    "    return r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{String,1}:\n",
       " \"3×3×1×1 AutoGrad.Param{KnetArray{Float32,4}}\"  \n",
       " \"4×4×1×1 AutoGrad.Param{KnetArray{Float32,4}}\"  \n",
       " \"5×5×1×1 AutoGrad.Param{KnetArray{Float32,4}}\"  \n",
       " \"1100×1802 AutoGrad.Param{KnetArray{Float32,2}}\"\n",
       " \"6×1100 AutoGrad.Param{KnetArray{Float32,2}}\"   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcnn =   Chain( Conv(3,3,1,1), \n",
    "                Conv(4,4,1,1),\n",
    "                Conv(5,5,1,1),\n",
    "                Dense(1802,1100,pdrop=0.5), \n",
    "                Dense(1100,6,pdrop=0.5))\n",
    "summary.(l.w for l in dcnn.layers)\n",
    "\n",
    "#                                                           y                 parameter                 memory\n",
    "                                                                                                            \n",
    "#input=300×450                                                                                       300x450\n",
    "#x1=300-3+1= 298/2 =149 ,y1= 450-3+1= /2 =  224   => y(149,224,5)              3x3x1x5               149x224x5\n",
    "#x2=149-4+1= 146/2 =73  ,y2= 224-4+1= /2=  110   => y(73,110,10)               4x4x5x10              73x110x10\n",
    "#x3=73-5+1=  69/2  =34  ,y2= 110-5+1= /2=  53    => y(34,53,10)                5x5x10x10             34x53x10\n",
    "#fc=34*53*10= 18.020                                                           5x5x10x18020           18.020\n",
    "#fc=1100x6=6600                                                                18020x1100              6600\n",
    "\n",
    "# Sum                                                                         ------------          ------------\n",
    "# #float variable                                                                                    424.820 x4\n",
    "#Byte                                                                                               =1.699.280\n",
    "#Batch memory B=160                                                                                 =271.884.800\n",
    "#consider add more dense layer, decide on where the dropout should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n",
      "1.79e+00  0.15%┣                               ┫ 1/680 [00:12/02:13:57, 11.82s/i]"
     ]
    },
    {
     "ename": "BoundsError",
     "evalue": "BoundsError: attempt to access 960-element UnitRange{Int32} at index [0]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 960-element UnitRange{Int32} at index [0]",
      "",
      "Stacktrace:",
      " [1] checkbetween at /home/ec2-user/.julia/packages/Knet/05UDD/src/karray.jl:641 [inlined]",
      " [2] checkbetween(::Array{Int32,1}, ::Int64, ::Int64) at /home/ec2-user/.julia/packages/Knet/05UDD/src/karray.jl:635",
      " [3] getindex(::KnetArray{Float32,2}, ::Array{Int64,1}) at /home/ec2-user/.julia/packages/Knet/05UDD/src/karray.jl:652",
      " [4] #nll#712(::Int64, ::Bool, ::Function, ::KnetArray{Float32,2}, ::Array{Int8,1}) at /home/ec2-user/.julia/packages/Knet/05UDD/src/loss.jl:221",
      " [5] nll(::KnetArray{Float32,2}, ::Array{Int8,1}) at /home/ec2-user/.julia/packages/Knet/05UDD/src/loss.jl:220",
      " [6] (::Chain)(::Array{Array{Any,2},1}, ::Array{Int8,1}) at ./In[19]:14",
      " [7] (::getfield(Main, Symbol(\"##20#21\")){Chain})(::Tuple{Array{Array{Any,2},1},Array{Int8,1}}) at ./none:0",
      " [8] mean(::typeof(identity), ::Base.Generator{Data{Tuple{Array{Array{Any,2},1},Array{Int8,1}}},getfield(Main, Symbol(\"##20#21\")){Chain}}) at ./generator.jl:47",
      " [9] mean at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Statistics/src/Statistics.jl:42 [inlined]",
      " [10] (::Chain)(::Data{Tuple{Array{Array{Any,2},1},Array{Int8,1}}}) at ./In[19]:17",
      " [11] (::getfield(Main, Symbol(\"##25#29\")){Chain})(::Float32) at ./none:0",
      " [12] iterate at ./generator.jl:47 [inlined]",
      " [13] iterate at ./iterators.jl:902 [inlined]",
      " [14] iterate at ./iterators.jl:898 [inlined]",
      " [15] _collect(::Type{Float32}, ::Base.Iterators.Flatten{Base.Generator{Base.Generator{Base.Iterators.Filter{getfield(Main, Symbol(\"##24#28\")){Int64},Base.Iterators.Enumerate{Knet.Progress{Knet.Minimize{Knet.Repeat}}}},getfield(Main, Symbol(\"##23#27\"))},getfield(Main, Symbol(\"##25#29\")){Chain}}}, ::Base.SizeUnknown) at ./array.jl:509",
      " [16] collect(::Type{Float32}, ::Base.Iterators.Flatten{Base.Generator{Base.Generator{Base.Iterators.Filter{getfield(Main, Symbol(\"##24#28\")){Int64},Base.Iterators.Enumerate{Knet.Progress{Knet.Minimize{Knet.Repeat}}}},getfield(Main, Symbol(\"##23#27\"))},getfield(Main, Symbol(\"##25#29\")){Chain}}}) at ./array.jl:503",
      " [17] #trainresults#22(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::Function, ::String, ::Chain) at ./In[20]:11",
      " [18] trainresults(::String, ::Chain) at ./In[20]:7",
      " [19] top-level scope at In[22]:1"
     ]
    }
   ],
   "source": [
    "cnn = trainresults(\"dcnn.jld2\", dcnn);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "using Plots; default(fmt=:png,ls=:auto)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison to MLP shows faster convergence, better generalization\n",
    "plot([cnn[1,:], cnn[2,:]],ylim=(0.0,0.1),\n",
    "     labels=[:trnCNN :tstCNN],xlabel=\"Epochs\",ylabel=\"Loss\")  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "plot([cnn[3,:], cnn[4,:]],ylim=(0.0,0.03),\n",
    "    labels=[:trnCNN :tstCNN],xlabel=\"Epochs\",ylabel=\"Error\")  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temp1=param(3,3,1,5)\n",
    "temp2=param(4,4,5,10);\n",
    "temp3=param(5,5,10,10);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sonW= W[:,permutedims(hcat(Iterators.first(dtrn)[1]...))]  #450*160=72000\n",
    "sonW=KnetArray{Float32}(reshape(sonW,(300,450,1,160)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "finalLayer=pool(conv4(temp3,pool(conv4(temp2,pool(conv4(temp1, dropout(sonW,0.5)))))));\n",
    "size(finalLayer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d=Dense(18020,1100,pdrop=0.5);\n",
    "xxxx=d(finalLayer);size(xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dcnn =   Chain(Conv(3,1,1,20), \n",
    "                Conv(4,1,20,50),\n",
    "                Conv(5,1,50,100),\n",
    "                Dense(3400,1100,pdrop=0.5), \n",
    "                Dense(1100,6,pdrop=0.5))\n",
    "summary.(l.w for l in dcnn.layers)\n",
    "\n",
    "                            y                 parameter                 memory\n",
    "input=300  \n",
    "y1=300-3+1= 298/2 =149 => y(149,1,20)             3x3x1x20               149x2x20          \n",
    "y2=149-4+1= 146/2 =73 => y(73,1,50)              4x4x20x50              73x1x50\n",
    "y3=73-5+1=  69/2  =34 => y(34,1,100)            5x5x50x100             34x1x100\n",
    "\n",
    "fc=3400\n",
    "\n",
    "#consider add more dense layer, decide on where the dropout should be "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dcnn =   Chain( Conv(3,3,1,1), \n",
    "                Conv(4,4,1,1),\n",
    "                Conv(5,5,1,1),\n",
    "                Dense(3059898,1100,pdrop=0.5), \n",
    "                Dense(1100,6,pdrop=0.5))\n",
    "summary.(l.w for l in dcnn.layers)\n",
    "\n",
    "#                                                           y                 parameter                 memory\n",
    "\n",
    "#input=300×72000                                                                                       \n",
    "#x1=300-3+1= 298/2 =149 ,y1= 720000-3+1= /2 =  359.999   => y(149,359.999,1)          \n",
    "#x2=149-4+1= 146/2 =73  ,y2= 359.999-4+1= /2=  179.998   => y(73,179.998,1) \n",
    "#x3=73-5+1=  69/2  =34  ,y2= 179.998-5+1= /2=  89.997    => y(34,89.997,1)\n",
    "#fc=34*89.997*1= 3.059.898\n",
    "\n",
    "#consider add more dense layer, decide on where the dropout should be \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "struct SequenceClassifier; input; cnn; dense ;output; pdrop; end"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SequenceClassifier(W,input::Int, embed::Int, hidden::Int, output::Int; pdrop=0) =\n",
    "    SequenceClassifier(param(W), RNN(embed,hidden,rnnType=:gru), param(output,hidden), pdrop)\n",
    "    #SequenceClassifier(param(embed,input), RNN(embed,hidden,rnnType=:gru), param(output,hidden), pdrop)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function (sc::SequenceClassifier)(input)\n",
    "    embed = sc.input[:, permutedims(hcat(input...))]\n",
    "    embed = dropout(embed,sc.pdrop)\n",
    "    hidden = sc.rnn(embed)\n",
    "    hidden = dropout(hidden,sc.pdrop)\n",
    "    return sc.output * hidden[:,:,end]\n",
    "end\n",
    "\n",
    "(sc::SequenceClassifier)(input,output) = nll(sc(input),output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@show size(xtrn)\n",
    "@show size(ytrn)\n",
    "dtrn = minibatch(xtrn,ytrn,BATCHSIZE;shuffle=true)\n",
    "dtst = minibatch(xtst,ytst,BATCHSIZE)\n",
    "length.((dtrn,dtst))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For running experiments\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        progress!(adam(model,repeat(dtrn,EPOCHS);lr=LR,beta1=BETA_1,beta2=BETA_2,eps=EPS))\n",
    "        Knet.save(file,\"model\",model)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "        model = Knet.load(file,\"model\")\n",
    "    end\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = SequenceClassifier(VOCABSIZE,EMBEDSIZE,NUMHIDDEN,NUMCLASS,pdrop=DROPOUT)\n",
    "nll(model,dtrn), nll(model,dtst), accuracy(model,dtrn), accuracy(model,dtst)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 2.51e-01  100.00%┣████████████████████┫ 1170/1170 [00:16/00:16, 75.46i/s]\n",
    "model = trainresults(\"imdbmodel113.jld2\",model);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# (0.059155148f0, 0.3877507f0, 0.9846153846153847, 0.8583733974358975)\n",
    "nll(model,dtrn), nll(model,dtst), accuracy(model,dtrn), accuracy(model,dtst)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
