{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Abdulrahman Alabrash \n",
    "\n",
    "https://github.com/alabrashJr/DCNN-Julia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for DCN\n",
    "    * build_data_cv :taking The data and its labels in addition to split dictionary which determine which tuple is train or test data, returns, \n",
    "                * revs, which is a list of datum  = {\"y\": \n",
    "                                    \"text\":,                             \n",
    "                                    \"num_words\": ,\n",
    "                                    \"split\": \n",
    "                             }\n",
    "                * Voca defaultdict which indicates the number of each word occurrence \n",
    "\n",
    "    * sibling2, take sentence and return a sibling list for each word sibling dependency list. The list elements will be as following.\n",
    "            * [1] Word \n",
    "            * [2] Parent \n",
    "            * [3] sibling1: \n",
    "                    * If word index < parent index:\n",
    "                            *  look for sibling indices < word index \n",
    "                                    * If there are not add  “Start”\n",
    "                    * If word index > parent index:\n",
    "                            * Look for sibling indices>word index \n",
    "                                    * If there are not add  “Stop”\n",
    "            * [4] Sibling2:\n",
    "                    *  If word index < parent index:\n",
    "                            *  look for sibling indices < word index \n",
    "                                    * If there are not add  “Start”\n",
    "                    * If word index > parent index:\n",
    "                            * Look for sibling indices>word index \n",
    "                                    * If there are not add  “Stop”\n",
    "            * [5] grand parent: \n",
    "                    * If is available add it if not add “Root”\n",
    "\n",
    "\n",
    "    * set_sibling2, execute sibling2 method for each sentence and padding it to the maxl which is 45, and add the label of the sentence as a list so the final length will be 46, for each sentence: size(#sentence(maxl(5))\n",
    "\n",
    "\n",
    "    * set_conv_sent, extract the 4 ancestors of the word, padding it to the maxl which is 45,and add the label of sentence as list so the final length will be 46, the returned value will be equal to header list + following list for each word In each sentence : size(#sentence(maxl(5))\n",
    "        * For each sentence \n",
    "        * [5 x Root]\n",
    "        * [4x Root,1st Word]\n",
    "        * [3xRoot,1stWord,1st ancestor]\n",
    "        * [2xRoot,1stWord,1st:2rd ancestor]\n",
    "        * [Root,1stWord,1st:3rd ancestor]\n",
    "        * For each word \n",
    "            * [ Word, 1st ancestor, 2nd ancestor, 3rd ancestor, 4th ancestor]\n",
    "\n",
    "\n",
    "* revs= Dict{String,Any} with 5 entries:\n",
    "        y-> label of the questions 1-5\n",
    "        num_words-> length of questions\n",
    "        tree -> concrete  the ancestors array with siblings array -> length of output array will be(#sentence(maxl(5+5))\n",
    "        text -> the question text\n",
    "        split -> type of tuple (training, test , div) \n",
    "\n",
    "\n",
    "* W = word embedding using google2vec  size=10097×300\n",
    "\n",
    "* W2= word emeding using uniform dist between -0,25 <-> 0,25    size=10097×300\n",
    "\n",
    "* word_idx_map= word indices in W matrices len=10097\n",
    "\n",
    "* vocab= vocab defalut Dic {word,number of occurence}  len=10097\n",
    "\n",
    "[revs, W, W2, word_idx_map, vocab] -> TREC_sib.jld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.1/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using Pkg;Pkg.update()\n",
    "for p in (\"Embeddings\",\"DataStructures\",\"DataFrames\",\"FileIO\",\"LinearAlgebra\",\"FileIO\"); haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using DataStructures,DataFrames,FileIO,Embeddings,LinearAlgebra,FileIO;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct node \n",
    "    word\n",
    "    kidsword\n",
    "    kidsindex\n",
    "    parent\n",
    "    finished\n",
    "    is_word\n",
    "    selfindex\n",
    "    parentindex\n",
    "    label\n",
    "    ind\n",
    "node(word) = word == nothing ? new(nothing,nothing,nothing,nothing,nothing,0,nothing,nothing,nothing,nothing) : new(word,[],[],[],0,1,0,0,\"\",-1)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_split (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_split(size=5953)\n",
    "dict=Dict()\n",
    "    for i in range(1,length=size)\n",
    "        if i < 5453\n",
    "            dict[i] = 1\n",
    "        else\n",
    "            dict[i] =2\n",
    "        end \n",
    "    end \n",
    "    return dict\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_labels (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_labels(fn)\n",
    "    f=open(fn,\"r\")\n",
    "    dict=Dict()\n",
    "    for (index, i) in enumerate(readlines(f))\n",
    "        dict[index] = parse(Int,i) \n",
    "    end\n",
    "    return dict\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_str (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function clean_str(string, TREC=false)\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    \n",
    "    string = replace(string,r\"[^A-Za-z0-9(),!?\\'\\`]\" =>s\" \")\n",
    "    string = replace(string,r\"\\'s\" =>s\" 's\") \n",
    "    string = replace(string,r\"\\'ve\" =>s\" 've\") \n",
    "    string = replace(string,r\"n\\'t\" =>s\" n't\") \n",
    "    string = replace(string,r\"\\'re\" =>s\" 're\") \n",
    "    string = replace(string,r\"\\'d\" =>s\" 'd\") \n",
    "    string = replace(string,r\"\\'ll\" =>s\" 'll\") \n",
    "    string = replace(string,r\",\" =>s\" , \") \n",
    "    string = replace(string,r\"!\" =>s\" ! \") \n",
    "    string = replace(string,r\"\\(\" =>s\" \\\\( \") \n",
    "    string = replace(string,r\"\\)\" =>s\" \\\\) \") \n",
    "    string = replace(string,r\"\\?\" =>s\" \\\\? \") \n",
    "    string = replace(string,r\"\\s{2,}\" =>s\" \")    \n",
    "        \n",
    "    return (TREC ?  strip(string) : lowercase(strip(string)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_data_cv (generic function with 2 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# revs: Creats datum Dictionary which contains each data tuple's label,question text, length of question text,type of tuple(train-test-dev)\n",
    "# vocab: default Dict ,counts counts the words occusions through all tuples\n",
    "function build_data_cv(file, split_dict, label_dict, clean_string=false)\n",
    "    \"\"\"\n",
    "    Loads data and split data\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    f = open(file,\"r\")\n",
    "    vocab = DefaultDict(0)#https://juliacollections.github.io/DataStructures.jl/latest/default_dict.html\n",
    "    \n",
    "    for (index, line) in enumerate(readlines(f))     \n",
    "        rev = []\n",
    "        push!(rev,strip(line))\n",
    "        if clean_string\n",
    "            orig_rev = clean_str(join(rev,\" \"))\n",
    "        else\n",
    "            orig_rev = join(rev,\" \")\n",
    "        end\n",
    "        words = Set(split(orig_rev))\n",
    "        for word in words\n",
    "            vocab[word] += 1\n",
    "        end\n",
    "        datum  = Dict(\"y\"=>label_dict[index], \n",
    "                    \"text\"=> orig_rev,                             \n",
    "                    \"num_words\"=> length(split(orig_rev)),\n",
    "                    \"split\"=> split_dict[index])\n",
    "        push!(revs,datum)\n",
    "    end\n",
    "\n",
    "    return revs, vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting each object of the tree to structured list of trees. \n",
    "<img width=\"657\" alt=\"Screen Shot 2019-04-02 at 15 45 50\" src=\"https://user-images.githubusercontent.com/9295206/55403412-77ff0800-555e-11e9-9cc7-e8977c06cb31.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sibling2 (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pic up  \n",
    "function sibling2(sents, opt)\n",
    "    sent_list = []\n",
    "    kez= sort(collect(keys(sents)))\n",
    "    for key in kez\n",
    "        currnet_node=sents[key]\n",
    "        if key == 0;continue;end\n",
    "        #currnet_node = sents[key]\n",
    "        word_list = []\n",
    "        push!(word_list,currnet_node.word)\n",
    "        \n",
    "        parent_index = currnet_node.parentindex\n",
    "        parent = sents[parent_index]\n",
    "        push!(word_list,parent.word)\n",
    "        sib_list = parent.kidsindex\n",
    "        if key < parent_index\n",
    "            sib_candidate = [i for i in sib_list if i < key]\n",
    "            if sib_candidate == [];push!(word_list,\"*START*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word);end \n",
    "            if sib_candidate == [];push!(word_list,\"*START*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word);end\n",
    "        else\n",
    "            sib_candidate = [i for i in sib_list if i > key]\n",
    "            if sib_candidate == [];push!(word_list,\"*STOP*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word);end\n",
    "            if sib_candidate == [];push!(word_list,\"*STOP*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word); end\n",
    "       end\n",
    "        grad_parent_ind = parent.parentindex\n",
    "        grad_word = sents[grad_parent_ind].word\n",
    "        push!(word_list,grad_word)\n",
    "        push!(sent_list,word_list)\n",
    "    end\n",
    "    return sent_list\n",
    " end\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_sibling2 (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creats lists of strcutred trees + padding till max =45 + label list \n",
    "function set_sibling2(tree,labels_dict,max_len)\n",
    "\n",
    "    sent_num = length(tree)\n",
    "    doc_list =[]\n",
    "    for (ind,sents) in enumerate(tree)\n",
    "        sib_6 = sibling2(sents,6)\n",
    "        sent_list = sib_6\n",
    "        dummy_len = length(sent_list[1])\n",
    "        dummy = repeat([\"*ZERO*\"],dummy_len)\n",
    "        while length(sent_list) < max_len #padding tree to the maximum tree by adding zeros list to sent_lists\n",
    "            push!(sent_list,dummy)\n",
    "        end\n",
    "        currnet_label = labels_dict[ind]\n",
    "        class_dummy = repeat([currnet_label],dummy_len)\n",
    "        push!(sent_list,class_dummy)        \n",
    "        push!(doc_list,sent_list)# adding the list\n",
    "        \n",
    "    end\n",
    "    return doc_list    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_conv_sent (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function set_conv_sent(tree,labels_dict,max_len)\n",
    "    conv_length = 5\n",
    "    @show sent_num = length(tree)\n",
    "    ##65-4 the most beginning 4 will be append to the front at last\n",
    "    #sent_tensor = np.array.zeros((1,61,5))\n",
    "    #sent_counter = 0\n",
    "    doc_list =[]\n",
    "    for (ind,sents) in enumerate(tree)\n",
    "         sent_list = []\n",
    "        kez= sort(collect(keys(sents)))\n",
    "        for key in kez\n",
    "            #@show key \n",
    "            if key == 0;continue;end\n",
    "            currnet_node = sents[key]\n",
    "            word_list = []\n",
    "            for i in range(1,conv_length)\n",
    "                #@show currnet_node.word\n",
    "                if currnet_node.word != \"ROOT\";push!(word_list,currnet_node.word)\n",
    "                else; push!(word_list,currnet_node.word);end\n",
    "                if currnet_node.word != \"ROOT\"; currnet_node = sents[currnet_node.parentindex];end\n",
    "            end\n",
    "             push!(sent_list,word_list)\n",
    "            #@show length(sent_list)\n",
    "        end \n",
    "        header = []\n",
    "        dummy = repeat([\"ROOT\"],conv_length)\n",
    "        for i in range(1,conv_length-1);push!(header,vcat(dummy[1:conv_length-i], sent_list[1][1:i]));end\n",
    "        sent_list = vcat(header,sent_list)\n",
    "        while length(sent_list) < max_len;push!(sent_list,dummy);end\n",
    "        currnet_label = labels_dict[ind]\n",
    "        class_dummy = repeat([currnet_label],conv_length)\n",
    "        push!(sent_list,class_dummy)\n",
    "        #@show length(sent_list)\n",
    "        push!(doc_list,sent_list)\n",
    "            end     \n",
    "   # @show length(doc_list)\n",
    "    return doc_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_tree2vocab (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add_tree2vocab(sent, vocab)\n",
    "    \n",
    "    for (j, each_word) in enumerate(sent[1:end-1])\n",
    "        for (l, each_field) in enumerate(each_word)\n",
    "            if each_field in keys(vocab);continue\n",
    "            elseif each_field == 0;continue\n",
    "            elseif each_field == \"ROOT\";continue\n",
    "                else;vocab[each_field] += 1;end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merge_two (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function merge_two(revs, tree)\n",
    "    counter=1\n",
    "    for i in revs\n",
    "        sent2 = tree[counter]\n",
    "        counter += 1\n",
    "        i[\"tree\"] = sent2\n",
    "    end    \n",
    "    return revs\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_bin_vec (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_bin_vec(fname, vocab)\n",
    "pf(s)=return parse(Int,s)\n",
    "pc(s)=return convert(Char,s[1])\n",
    "word_vecs = Dict()\n",
    "    open(fname, \"r\") do f\n",
    "                @show header = readline(f)\n",
    "                vocab_size, layer1_size = map(pf, split(header))\n",
    "                @show binary_len = sizeof(Float32) * layer1_size\n",
    "                #@show  binary_len = layer1_size\n",
    "             for line in collect(1:vocab_size)\n",
    "                word=[]\n",
    "            #println(\"enter to while \\t\")    \n",
    "                while true \n",
    "                           ch=read(f,1)\n",
    "                           ch=convert(Char,ch[1])\n",
    "                            if ch == ' '\n",
    "                                word = join(word,\"\")\n",
    "                                break\n",
    "                            end\n",
    "                            if ch != '\\n';\n",
    "                                push!(word,ch);\n",
    "                            end\n",
    "                    end\n",
    "       # println(\"Exiting from while \\t\")    \n",
    "                if word in keys(vocab)\n",
    "                   word_vecs[word]=reinterpret(Float32,read(f,binary_len))\n",
    "#                    w = Array{Float64}(map(pc,temp))\n",
    "#                    vec_norm = norm(w)\n",
    "#                    word_vecs[word]= w./vec_norm\n",
    "                else\n",
    "                read(f,binary_len)\n",
    "                end\n",
    "\n",
    "            end    \n",
    "    end;\n",
    "return word_vecs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_unknown_words (generic function with 3 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add_unknown_words(word_vecs, vocab, min_df=1, k=300)\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for (word,w) in vocab\n",
    "        if word ∉ keys(word_vecs) && vocab[word] >= min_df\n",
    "            word_vecs[word] = (rand(k).*0.5).- 0.25\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_W (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_W(word_vecs, k=300)\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = length(word_vecs)\n",
    "    word_idx_map = Dict()\n",
    "    W = zeros((vocab_size+1, k))            \n",
    "    W[1,:] =  zeros(300)\n",
    "    i = 1\n",
    "    for (word,w) in word_vecs\n",
    "        W[i,:] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    end\n",
    "    return W, word_idx_map\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_num = length(tree) = 5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `range(start, stop)` (with neither `length` nor `step` given) is deprecated, use `range(start, stop=stop)` instead.\n",
      "│   caller = set_conv_sent(::Array{Any,1}, ::Dict{Any,Any}, ::Int64) at In[9]:16\n",
      "└ @ Main ./In[9]:16\n",
      "┌ Warning: `range(start, stop)` (with neither `length` nor `step` given) is deprecated, use `range(start, stop=stop)` instead.\n",
      "│   caller = set_conv_sent(::Array{Any,1}, ::Dict{Any,Any}, ::Int64) at In[9]:27\n",
      "└ @ Main ./In[9]:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length(vocab) = 10096\n"
     ]
    }
   ],
   "source": [
    "w2v_file = \"google_w2v.bin\"   \n",
    "sent_file = \"Data/TREC_all.txt\"\n",
    "#tree_file = \"Data/data.jld2\" # hdf5 wrtoe \n",
    "label_file = \"Data/label_all.txt\"\n",
    "label_dict = get_labels(label_file);\n",
    "split_dict = get_split(5952) ;\n",
    "       \n",
    "revs, vocab = build_data_cv(sent_file, split_dict, label_dict); \n",
    "function dfun(d::Dict);return d[\"num_words\"];end\n",
    "max_l,maxIndex = findmax(map(dfun, revs)) # find the longest text length \n",
    "    \n",
    "all_tree = load(\"Data/data.jld2\",\"data\"); # load node objects that have been created in pre-indexing file \n",
    "data_sibling = set_sibling2(all_tree,label_dict,max_l+8);\n",
    "data_tree = set_conv_sent(all_tree,label_dict,max_l+8); \n",
    "#summary.(data_tree)\n",
    "\n",
    "new_data_tree = []\n",
    "for (ind,l) in enumerate(data_tree)\n",
    "    new_list=[]\n",
    "    for (ind2,l2) in enumerate(l);push!(new_list,vcat(data_tree[ind][ind2],data_sibling[ind][ind2]));end\n",
    "    push!(new_data_tree,new_list)\n",
    "end\n",
    "data_tree = new_data_tree\n",
    "#@show length.(new_data_tree)\n",
    "for i in data_tree;add_tree2vocab(i, vocab);end\n",
    "@show length(vocab)\n",
    "revs = merge_two(revs,data_tree);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How did serfdom develop in and then leave Russia ?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46-element Array{Any,1}:\n",
       " Any[\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"How\", \"How\", \"develop\", \"*START*\", \"*START*\", \"ROOT\"]         \n",
       " Any[\"ROOT\", \"ROOT\", \"ROOT\", \"How\", \"develop\", \"did\", \"develop\", \"How\", \"*START*\", \"ROOT\"]          \n",
       " Any[\"ROOT\", \"ROOT\", \"How\", \"develop\", \"ROOT\", \"serfdom\", \"develop\", \"How\", \"did\", \"ROOT\"]          \n",
       " Any[\"ROOT\", \"How\", \"develop\", \"ROOT\", \"ROOT\", \"develop\", \"ROOT\", \"*STOP*\", \"*STOP*\", \"ROOT\"]       \n",
       " Any[\"How\", \"develop\", \"ROOT\", \"ROOT\", \"ROOT\", \"in\", \"develop\", \"and\", \"leave\", \"ROOT\"]             \n",
       " Any[\"did\", \"develop\", \"ROOT\", \"ROOT\", \"ROOT\", \"and\", \"develop\", \"leave\", \"?\", \"ROOT\"]              \n",
       " Any[\"serfdom\", \"develop\", \"ROOT\", \"ROOT\", \"ROOT\", \"then\", \"leave\", \"*START*\", \"*START*\", \"develop\"]\n",
       " Any[\"develop\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"leave\", \"develop\", \"?\", \"*STOP*\", \"ROOT\"]          \n",
       " Any[\"in\", \"develop\", \"ROOT\", \"ROOT\", \"ROOT\", \"Russia\", \"leave\", \"*STOP*\", \"*STOP*\", \"develop\"]     \n",
       " Any[\"and\", \"develop\", \"ROOT\", \"ROOT\", \"ROOT\", \"?\", \"develop\", \"*STOP*\", \"*STOP*\", \"ROOT\"]          \n",
       " Any[\"then\", \"leave\", \"develop\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]  \n",
       " Any[\"leave\", \"develop\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]  \n",
       " Any[\"Russia\", \"leave\", \"develop\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]\n",
       " ⋮                                                                                                  \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [\"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"ROOT\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\", \"*ZERO*\"]         \n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]                                                                     "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[1][\"tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 5952\n",
      "vocab size: 10096\n",
      "max sentence length: 45\n",
      "loading word2vec vectors...\n",
      "header = readline(f) = \"3000000 300\"\n",
      "binary_len = sizeof(Float32) * layer1_size = 1200\n",
      "word2vec loaded!\n",
      "num words already in word2vec: 9040\n",
      "num words already in word2vec: 10097\n",
      "dataset created!\n"
     ]
    }
   ],
   "source": [
    "println(\"number of sentences: \", length(revs))\n",
    "println(\"vocab size: \" ,length(vocab))\n",
    "println(\"max sentence length: \" ,max_l+8)\n",
    "println(\"loading word2vec vectors...\")\n",
    "w2v = load_bin_vec(w2v_file, vocab)\n",
    "#w2v=Dict()\n",
    "println(\"word2vec loaded!\")\n",
    "println(\"num words already in word2vec: \",length(w2v))\n",
    "vocab[\"ROOT\"]=1\n",
    "vocab[\"*START*\"]=1\n",
    "vocab[\"*STOP*\"]=1\n",
    "vocab[\"*ZERO*\"]=1   \n",
    "add_unknown_words(w2v, vocab)\n",
    "println(\"num words already in word2vec: \",length(w2v))\n",
    "W, word_idx_map = get_W(w2v)\n",
    "rand_vecs = Dict()\n",
    "add_unknown_words(rand_vecs, vocab)\n",
    "W2, _ = get_W(rand_vecs)\n",
    "save(\"Data/TREC_sib.jld2\",\"datas\",[revs, W, W2, word_idx_map, vocab])  \n",
    "println(\"dataset created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
