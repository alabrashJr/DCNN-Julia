{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; for p in (\"Embeddings\",\"DataStructures\",\"DataFrames\",\"FileIO\",\"LinearAlgebra\",\"Knet\",\"FileIO\"); haskey(Pkg.installed(),p) || Pkg.add(p); end\n",
    "using DataStructures,DataFrames,FileIO,Embeddings,LinearAlgebra,FileIO\n",
    "using Knet: Knet, AutoGrad, param, param0, mat, RNN, relu, Data, adam, progress, nll, zeroone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct node \n",
    "    word\n",
    "    kidsword\n",
    "    kidsindex\n",
    "    parent\n",
    "    finished\n",
    "    is_word\n",
    "    selfindex\n",
    "    parentindex\n",
    "    label\n",
    "    ind\n",
    "node(word) = word == nothing ? new(nothing,nothing,nothing,nothing,nothing,0,nothing,nothing,nothing,nothing) : new(word,[],[],[],0,1,0,0,\"\",-1)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_split(size=5952)\n",
    "dict=Dict()\n",
    "    for i in range(1,length=size)\n",
    "        if i < 5452\n",
    "            dict[i] = 1\n",
    "        else\n",
    "            dict[i] =2\n",
    "        end \n",
    "    end \n",
    "    return dict\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_labels(fn)\n",
    "    f=open(fn,\"r\")\n",
    "    dict=Dict()\n",
    "    for (index, i) in enumerate(readlines(f))\n",
    "        dict[index] = parse(Int,i) \n",
    "    end\n",
    "    return dict\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function clean_str(string, TREC=false)\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    \n",
    "    string = replace(string,r\"[^A-Za-z0-9(),!?\\'\\`]\" =>s\" \")\n",
    "    string = replace(string,r\"\\'s\" =>s\" 's\") \n",
    "    string = replace(string,r\"\\'ve\" =>s\" 've\") \n",
    "    string = replace(string,r\"n\\'t\" =>s\" n't\") \n",
    "    string = replace(string,r\"\\'re\" =>s\" 're\") \n",
    "    string = replace(string,r\"\\'d\" =>s\" 'd\") \n",
    "    string = replace(string,r\"\\'ll\" =>s\" 'll\") \n",
    "    string = replace(string,r\",\" =>s\" , \") \n",
    "    string = replace(string,r\"!\" =>s\" ! \") \n",
    "    string = replace(string,r\"\\(\" =>s\" \\\\( \") \n",
    "    string = replace(string,r\"\\)\" =>s\" \\\\) \") \n",
    "    string = replace(string,r\"\\?\" =>s\" \\\\? \") \n",
    "    string = replace(string,r\"\\s{2,}\" =>s\" \")    \n",
    "        \n",
    "    return (TREC ?  strip(string) : lowercase(strip(string)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_data_cv(file, split_dict, label_dict, clean_string=false)\n",
    "    \"\"\"\n",
    "    Loads data and split data\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    f = open(file,\"r\")\n",
    "    vocab = DefaultDict(0)#https://juliacollections.github.io/DataStructures.jl/latest/default_dict.html\n",
    "    \n",
    "    for (index, line) in enumerate(readlines(f))     \n",
    "        rev = []\n",
    "        push!(rev,strip(line))\n",
    "        if clean_string\n",
    "            orig_rev = clean_str(join(rev,\" \"))\n",
    "        else\n",
    "            orig_rev = join(rev,\" \")\n",
    "        end\n",
    "        words = Set(split(orig_rev))\n",
    "        for word in words\n",
    "            vocab[word] += 1\n",
    "        end\n",
    "        datum  = Dict(\"y\"=>label_dict[index], \n",
    "                    \"text\"=> orig_rev,                             \n",
    "                    \"num_words\"=> length(split(orig_rev)),\n",
    "                    \"split\"=> split_dict[index])\n",
    "        push!(revs,datum)\n",
    "    end\n",
    "\n",
    "    return revs, vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sibling2(sents, opt)\n",
    "    sent_list = []\n",
    "    \n",
    "    for (key,currnet_node) in sents\n",
    "        #@show key\n",
    "        #@show currnet_node\n",
    "        if key == 0;continue;end\n",
    "        #currnet_node = sents[key]\n",
    "        word_list = []\n",
    "        push!(word_list,currnet_node.word)\n",
    "        \n",
    "        parent_index = currnet_node.parentindex\n",
    "        parent = sents[parent_index]\n",
    "        push!(word_list,parent.word)\n",
    "        sib_list = parent.kidsindex\n",
    "        if key < parent_index\n",
    "            sib_candidate = [i for i in sib_list if i < key]\n",
    "            if sib_candidate == [];push!(word_list,\"*START*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word);end \n",
    "            if sib_candidate == [];push!(word_list,\"*START*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word);end\n",
    "        else\n",
    "            sib_candidate = [i for i in sib_list if i > key]\n",
    "            if sib_candidate == [];push!(word_list,\"*STOP*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word);end\n",
    "            if sib_candidate == [];push!(word_list,\"*STOP*\")\n",
    "            else;push!(word_list,sents[pop!(sib_candidate)].word); end\n",
    "       end\n",
    "        grad_parent_ind = parent.parentindex\n",
    "        grad_word = sents[grad_parent_ind].word\n",
    "        push!(word_list,grad_word)\n",
    "        push!(sent_list,word_list)\n",
    "    end\n",
    "    return sent_list\n",
    "end\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_sibling2(tree,labels_dict,max_len)\n",
    "\n",
    "    sent_num = length(tree)\n",
    "    doc_list =[]\n",
    "    for (ind,sents) in enumerate(tree)\n",
    "        #sents\n",
    "        sib_6 = sibling2(sents,6)\n",
    "        sent_list = sib_6\n",
    "        dummy_len = length(sent_list[1])\n",
    "        dummy = repeat([\"*ZERO*\"],dummy_len)\n",
    "        while length(sent_list) < max_len\n",
    "            push!(sent_list,dummy)\n",
    "        end\n",
    "        currnet_label = labels_dict[ind]\n",
    "        class_dummy = [currnet_label]*dummy_len\n",
    "        push!(sent_list,class_dummy)        \n",
    "        push!(doc_list,sent_list)\n",
    "    end\n",
    "    return doc_list    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function set_conv_sent(tree,labels_dict,max_len)\n",
    "    conv_length = 5\n",
    "    @show sent_num = length(tree)\n",
    "    ##65-4 the most beginning 4 will be append to the front at last\n",
    "    #sent_tensor = np.array.zeros((1,61,5))\n",
    "    #sent_counter = 0\n",
    "    doc_list =[]\n",
    "    for (ind,sents) in enumerate(tree)\n",
    "         sent_list = []\n",
    "        for (key,currnet_node) in sents\n",
    "            #@show key \n",
    "            if key == 0;continue;end\n",
    "            #currnet_node = sents[key]\n",
    "            word_list = []\n",
    "            for i in range(1,conv_length)\n",
    "                #@show currnet_node.word\n",
    "                if currnet_node.word != \"ROOT\";push!(word_list,currnet_node.word)\n",
    "                else; push!(word_list,currnet_node.word);end\n",
    "                if currnet_node.word != \"ROOT\"; currnet_node = sents[currnet_node.parentindex];end\n",
    "            end\n",
    "             push!(sent_list,word_list)\n",
    "            #@show length(sent_list)\n",
    "        end \n",
    "        header = []\n",
    "        dummy = repeat([\"ROOT\"],conv_length)\n",
    "        for i in range(1,conv_length-1);push!(header,vcat(dummy[1:conv_length-i-1], sent_list[1][1:i+1]));end\n",
    "        sent_list = vcat(header,sent_list)\n",
    "        while length(sent_list) < max_len;push!(sent_list,dummy);end\n",
    "        currnet_label = labels_dict[ind]\n",
    "        class_dummy = repeat([currnet_label],conv_length)\n",
    "        push!(sent_list,class_dummy)\n",
    "        #@show length(sent_list)\n",
    "        push!(doc_list,sent_list)\n",
    "            end     \n",
    "    @show length(doc_list)\n",
    "    return doc_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_tree2vocab(sent, vocab)\n",
    "    \n",
    "    for (j, each_word) in enumerate(sent[1:end-1])\n",
    "        for (l, each_field) in enumerate(each_word)\n",
    "            if each_field in keys(vocab);continue\n",
    "            elseif each_field == 0;continue\n",
    "            elseif each_field == \"ROOT\";continue\n",
    "                else;vocab[each_field] += 1;end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function merge_two(revs, tree)\n",
    "    counter=1\n",
    "    for i in revs\n",
    "        sent2 = tree[counter]\n",
    "        counter += 1\n",
    "        i[\"tree\"] = sent2\n",
    "    end    \n",
    "    return revs\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : adjust this function \n",
    "function load_bin_vec(fname, vocab)\n",
    "p(s)=return parse(Int,s)\n",
    "    open(fname, \"r\") do f\n",
    "            @show header = readline(f)\n",
    "            vocab_size, layer1_size = map(p, split(header))\n",
    "            @show binary_len = sizeof(Float32) * layer1_size\n",
    "        for line in collect(1:vocab_size)\n",
    "            word=[]\n",
    "            while true \n",
    "                    ch=read(f,1)\n",
    "                    if ch == ' ';word = join(word,\"\");break;end\n",
    "                    if ch != '\\n';push!(word,ch);end\n",
    "            end\n",
    "            if word in keys(vocab)\n",
    "                word_vecs[word] = Array{Float32}(map(pf,read(f,1200)))\n",
    "                vec_norm = norm(w)\n",
    "                word_vecs[word]= w./vec_norm\n",
    "            @show length(word_vecs[word])\n",
    "            else;read(f,binary_len);end\n",
    "        end    \n",
    "    end;\n",
    "return word_vecs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word ∉ word_vecs && vocab[word] >= min_df\n",
    "            word_vecs[word] = rand(k)*0.5 - 0.25 \n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = length(word_vecs)\n",
    "    word_idx_map = Dict()\n",
    "    W = zeros((vocab_size+1, k))            \n",
    "    W[1] = zeros(k)\n",
    "    i = 1\n",
    "    for word in word_vecs\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    end\n",
    "    return W, word_idx_map\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execfile(\"preindex.py\")\n",
    "w2v_file = \"/Users/abdulrhmanalabrash/Google Drive/KU/DL/paper/DCNN/data/google_w2v.bin\"   \n",
    "sent_file = \"/Users/abdulrhmanalabrash/Google Drive/KU/DL/paper/DCNN/TREC/TREC_all.txt\"\n",
    "tree_file = \"/Users/abdulrhmanalabrash/Google Drive/KU/DL/paper/DCNN/TREC/TREC_all_tree.jld2\" # hdf5 wrtoe \n",
    "label_file = \"/Users/abdulrhmanalabrash/Google Drive/KU/DL/paper/DCNN/TREC/label_all.txt\"\n",
    "label_dict = get_labels(label_file);\n",
    "split_dict = get_split(5952) ;\n",
    "       \n",
    "revs, vocab = build_data_cv(sent_file, split_dict, label_dict);\n",
    "function dfun(d::Dict);return d[\"num_words\"];end\n",
    "max_l,maxIndex = findmax(map(dfun, revs))\n",
    "    \n",
    "all_tree = load(\"data.jld2\",\"data\"); # use jld2 method\n",
    "data_sibling = set_sibling2(all_tree,label_dict,max_l+8);\n",
    "data_tree = set_conv_sent(all_tree,label_dict,max_l+8); \n",
    "#summary.(data_tree)\n",
    "\n",
    "new_data_tree = []\n",
    "for (ind,l) in enumerate(data_tree)\n",
    "    new_list=[]\n",
    "    for (ind2,l2) in enumerate(l);push!(new_list,vcat(data_tree[ind][ind2],data_sibling[ind][ind2]));end\n",
    "    push!(new_data_tree,new_list)\n",
    "end\n",
    "data_tree = new_data_tree\n",
    "#@show length.(new_data_tree)\n",
    "for i in data_tree;add_tree2vocab(i, vocab);end\n",
    "@show length(vocab)\n",
    "revs = merge_two(revs,data_tree);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    println(\"data loaded!\")\n",
    "    println(\"number of sentences: \", length(revs))\n",
    "    println(\"vocab size: \" ,length(vocab))\n",
    "    println(\"max sentence length: \" ,max_l)\n",
    "    println(\"loading word2vec vectors...\")\n",
    "   # w2v = load_bin_vec(w2v_file, vocab)\n",
    "    println(\"word2vec loaded!\")\n",
    "    #println(\"num words already in word2vec: \",length(w2v)))\n",
    "    vocab[\"ROOT\"]=1\n",
    "    vocab[\"*START*\"]=1\n",
    "    vocab[\"*STOP*\"]=1\n",
    "    vocab[\"*ZERO*\"]=1\n",
    "    vocab[\"*STARTWE*\"]=1\n",
    "    vocab[\"*STOPWE*\"]=1\n",
    "    vocab[\"*ZEROWE*\"]=1    \n",
    "#     add_unknown_words(w2v, vocab)\n",
    "#     W, word_idx_map = get_W(w2v)\n",
    "#     rand_vecs = Dict()\n",
    "#     add_unknown_words(rand_vecs, vocab)\n",
    "#     W2, _ = get_W(rand_vecs)\n",
    "#save(\"TREC_sib.jld2\",\"datas\",[revs, W, W2, word_idx_map, vocab])  \n",
    "#     println(\"dataset created!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
