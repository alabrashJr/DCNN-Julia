#@Abdulrahman Alabrash
# https://github.com/alabrashJr/DCNN-Julia
#
# Read data from saved file load("TREC_sib.jld2","datas")-> returns revs, W, W2, word_idx_map, vocab
#
# revs Dict{String,Any} with 5 entries:y,num_words,tree,text,split
# y-> label of the questions 1-5
# num_words-> length of questions
# tree -> 46-element Array{Array,1}, each array contains data_sibling(5)+data_tree(5) =10
# text -> the question text
# split -> type of tuple (training, test , div)
# W wordembedding generated by google2vec, size(10098×300)
# W2 word embedding generated by -0.25 , 0.25 size(10098×300)
# word_idx_map Dict{Any,Any} with 10097 entries , unique word id dictionary
# vocab DefaultDict{Any,Any,Int64} with 10097 entries, vocab and reptation dictionary


using Pkg;Pkg.update(); for p in ("DataStructures","LinearAlgebra","Knet","FileIO"); haskey(Pkg.installed(),p) || Pkg.add(p); end
using DataStructures,FileIO,LinearAlgebra;
using Base.Iterators: flatten
using Statistics: mean
using Knet
using Knet:Data
using Dates

#"abbreviation", "entity", "description", "location" ,"numeric","human"
Labels=["ENTY", "LOC", "ABBR", "NUM", "HUM", "DESC"]
revs, W, W2, word_idx_map, vocab=load("Data/TREC_sib.jld2","datas");
word_idx_map["ROOT"]=size(W,1);
W=W';

#Transforms sentence into a list of indices. Pad with zeroes.
function get_text_mat(t,word_idx_map;max_l=56,filter_h=5)
    #t the text of question
    x=[] # output matrix
    pad=filter_h -1 # padding number
    for i in collect(1:pad);push!(x,0);end #adding padding
    words=split(t)
    #extract the unique id of words in the question text and adding it to the matrix
    for w in words
        if w in keys(word_idx_map);push!(x,word_idx_map[w])
        else; @show w ;end
    end

    while length(x)<max_l+2*pad    # accomplish 64 +1 size by adding zeros till finish
            push!(x,0)
    end

    return  x
end


function getSen(vector)
#labels=["abbreviation","numeric",  "description", "human","location" ,"entity"]
    t=Array{Int}(vector)
    println(permutedims(t))
    for i in t
        if i==1557;print("?\n y =",t[end]+1 );break;end
        if i==0;continue;end
        for (key,value) in word_idx_map
            if value==i; print(key," ");end
        end
    end
end


#Transforms sentence into a list of indices. Pad with zeroes.
function get_tree_rep(r,word_idx_map)
# question
#@show t=r["tree"] #the tree of question
    each_sent=deepcopy(r)# output matrix
    for (j, each_word) in enumerate(each_sent[1:end-1])
        #@show (j, each_word)
            for (l, each_field) in enumerate(each_word)
           # @show (l, each_field)
                if each_field in keys( word_idx_map)
                #@show j,l ;
                    each_sent[j]=Array{Any,1}(each_sent[j])
                     each_sent[j][l] = word_idx_map[each_field]
                elseif each_field == 0
                    continue
                else
                    @show each_field
                end
            end
    end
    return each_sent;
end

function train_dev_test(revs)
    s1,s2,s3=[],[],[]
    t1,t2,t3=[],[],[]
    for rev in revs
    sent =get_text_mat(rev["text"], word_idx_map)
    push!(sent,rev["y"])
    sent_tensor = get_tree_rep(rev["tree"], word_idx_map)

    if rev["split"]==1
            push!(s1,Array{Int}(sent))
            push!(t1,sent_tensor)
    elseif rev["split"]==2
            push!(s2,Array{Int}(sent))
            push!(t2,sent_tensor)
    elseif rev["split"]==3
            push!(s3,Array{Int}(sent))
            push!(t3,sent_tensor)
    end
end

    train = hcat([f1 for f1 in s1]...)
    test =hcat([f1 for f1 in s2]...)
    dev = hcat([f1 for f1 in s3]...)
    train_tensor = t1
    test_tensor = t2
    dev_tensor = t3
    return (train,test,dev),(train_tensor,test_tensor,dev_tensor)
end

dataset,datasetTensor=train_dev_test(revs);

sent1=vcat([permutedims(vcat(datasetTensor[1][x][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]);#sent1=vcat([permutedims(vcat(datasetTensor[1][1][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]...)
sent2=vcat([permutedims(vcat(datasetTensor[2][x][1:end-1]...)) for x in 1:size(datasetTensor[2],1)]);#sent1=vcat([permutedims(vcat(datasetTensor[1][1][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]...);

y_train=Array{Int8}([dataset[1][:,x][end] for x in 1:size(dataset[1],2)]);#ytrainT=[datasetTensor[1][x,:][end][end][end] for x in 1:size(datasetTensor[1],1)];
y_test=Array{Int8}([dataset[2][:,x][end] for x in 1:size(dataset[2],2)]);#ytestT=[datasetTensor[2][x,:][end][end][end] for x in 1:size(datasetTensor[2],1)];

y_train=y_train.+1;
y_test=y_test.+1;

#input 5452 of 
reshape(W[:, permutedims(hcat(sent1[1]...))],(300,450,1,1));

dtrn=minibatch(sent1,y_train,160,shuffle=true);
dtst=minibatch(sent2,y_test,160);

#minibatch input
reshape(W[:, permutedims(hcat(first(dtrn)[1]...))],(300,450,1,160));

function guassian(w1,w2,cx,cy;range=[0.01,-0.01])
    return Knet.Param(KnetArray{Float32}((rand(w1,w2,cx,cy).*0.02).- 0.01))
end

struct Chain
    layers; λ1; λ2
    Chain(layers...; λ1=0, λ2=0) = new(layers, λ1, λ2)
end
function (c::Chain)(x)
    #x=KnetArray{Float32}(reshape(W[:, permutedims(hcat(x...))],(300,450,1,160)))
    x=KnetArray{Float32}(reshape(W[:, vec(hcat(x...))],(300,450,1,160)))
    all=vcat(c.layers[1](x),c.layers[2](x),c.layers[3](x))
    return c.layers[4](all)
    #(for l in c.layers; x = l(x); end; x)
    
end
(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)
function (c::Chain)(x,y)
    loss = nll(c(x),y)
    if training() # Only apply regularization during training, only to weights, not biases.
        c.λ1 != 0 && (loss += c.λ1 * sum(sum(abs, l.w) for l in c.layers))
        c.λ2 != 0 && (loss += c.λ2 * sum(sum(abs2,l.w) for l in c.layers))
    end
    return loss
end

# Define a convolutional layer:
struct Conv; w; b; f; p;E; end
function (c::Conv)(x)
    #return c.f.(pool(conv4(c.w, dropout(x,c.p)) .+ c.b))
    conv=conv4(c.w, dropout(x,c.p))
    #@show size(conv)
   # r=reshape(c.f.(pool(conv,window=size(conv)[2]).+c.b),(size(conv)[3],1,1,size(conv)[4]))
    r=reshape(c.f.(pool(conv,window=(1,size(conv)[2])).+c.b),(size(conv)[3],size(conv)[4]))
    #@show size(r)
    return r
end
#Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0,E=W) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop,E)
Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0,E=W) = Conv(guassian(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop,E)


# Redefine dense layer (See mlp.ipynb):
struct Dense; w; b; f; p; end
function (d::Dense)(x)
#     println("\nDensedeyim ,\t " , typeof(x),"\t", summary(x))
    d.f.(d.w * mat(dropout(x,d.p)) .+ d.b) # mat reshapes 4-D tensor to 2-D matrix so we can use matmul
end
Dense(i::Int,o::Int,f=Knet.identity;pdrop=0) = Dense(Knet.Param(KnetArray{Float32}((rand(o,i).*0.02).- 0.01)), param0(o), f, pdrop)



function trainresults(file,model; o...)
        println("lr =",lr_decay," \t n_epochs= ",n_epochs)
    if (print("Train from scratch? "); readline()[1]=='y')
        takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)
        r = ((model(dtrn), model(dtst), zeroone(model,dtrn), zeroone(model,dtst))
             for x in takeevery(length(dtrn), progress(adadelta(model,repeat(dtrn,n_epochs),lr=lr_decay))))
        r = reshape(collect(Float32,flatten(r)),(4,:))
        Knet.save(file,"results",r)
        Knet.gc() # To save gpu memory
    else
        if isfile(file);r=Knet.load(file,"results");else;println("there is no file such this");return;end
    end
    println(minimum(r,dims=2))
    return r
end

#input=300×450                                                                                       300x450
#x1=300-3+1= 298/2 =149 ,y1= 450-3+1= /2 =  224   => y(149,224,1)              3x3x1x5               149x224x5
#x2=149-4+1= 146/2 =73  ,y2= 224-4+1= /2=  110   => y(73,110,1)               4x4x5x10              73x110x10
#x3=73-5+1=  69/2  =34  ,y2= 110-5+1= /2=  53    => y(34,53,1)                5x5x10x10             34x53x10
#fc=34*53*10= 1.802                                                           5x5x10x18020           18.020
#fc=1100x6=6600                                                                18020x1100              6600
# dcnn7=Chain( Conv(3,3,1,100),
# Conv(4,4,100,100),
# Conv(5,5,100,100),
# Dense(180200,1100,pdrop=0.5),
# Dense(1100,6,pdrop=0.5),λ2=3)
# summary.(l.w for l in dcnn7.layers)

#n_epochs=80;
#lr_decay = 0.95
#cnn9=trainresults("models/dcnn13.jld2", dcnn7);

d=300
# w1=guassian(d,3,1,100)# w1 =reshape([1.0:1500.0...], (5,3,1,100));
# w2=guassian(d,4,1,100)# w2=reshape([1.0:2000.0...], (5,4,1,100));
# w3=guassian(d,5,1,100)
dene=Chain(Conv(d,3,1,100)
,Conv(d,4,1,100)
,Conv(d,5,1,100)
,Dense(300,6,pdrop=0.5),λ1=4f-6)
summary.(l.w for l in dene.layers)

n_epochs=20;
lr_decay = 0.95
cnn9=trainresults("models/dcnn13_2.jld2", dene);

#x=reshape(W[:, permutedims(hcat(first(dtrn)[1]...))],(300,450,1,160))
#t=Conv(d,3,1,100)
#t(x)
#all=KnetArray{Float32}([])
#all=vcat(dene.layers[1](x),dene.layers[2](x),dene.layers[3](x))

#den=Dense(300,6)

#out=den(all)

# first(dtrn)[1]

# l=Knet.@diff nll(out,y)

# @diff dene(first(dtrn)[1],y)

# -log(1/6)

