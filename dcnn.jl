#@Abdulrahman Alabrash
# https://github.com/alabrashJr/DCNN-Julia
#
# Read data from saved file load("TREC_sib.jld2","datas")-> returns revs, W, W2, word_idx_map, vocab
#
# revs Dict{String,Any} with 5 entries:y,num_words,tree,text,split
# y-> label of the questions 1-5
# num_words-> length of questions
# tree -> 46-element Array{Array,1}, each array contains data_sibling(5)+data_tree(5) =10
# text -> the question text
# split -> type of tuple (training, test , div)
# W wordembedding generated by google2vec, size(10098×300)
# W2 word embedding generated by -0.25 , 0.25 size(10098×300)
# word_idx_map Dict{Any,Any} with 10097 entries , unique word id dictionary
# vocab DefaultDict{Any,Any,Int64} with 10097 entries, vocab and reptation dictionary


using Pkg;Pkg.update(); for p in ("DataStructures","LinearAlgebra","Knet","FileIO"); haskey(Pkg.installed(),p) || Pkg.add(p); end
using DataStructures,FileIO,LinearAlgebra;
using Base.Iterators: flatten
using Statistics: mean
using Knet
using Knet:Data
using Dates

#"abbreviation", "entity", "description", "location" ,"numeric","human"
Labels=["ENTY", "LOC", "ABBR", "NUM", "HUM", "DESC"]
revs, W, W2, word_idx_map, vocab=load("Data/TREC_sib.jld2","datas");
word_idx_map["ROOT"]=size(W,1);
W=W';

#Transforms sentence into a list of indices. Pad with zeroes.
function get_text_mat(t,word_idx_map;max_l=56,filter_h=5)
    #t the text of question
    x=[] # output matrix
    pad=filter_h -1 # padding number
    for i in collect(1:pad);push!(x,0);end #adding padding
    words=split(t)
    #extract the unique id of words in the question text and adding it to the matrix
    for w in words
        if w in keys(word_idx_map);push!(x,word_idx_map[w])
        else; @show w ;end
    end

    while length(x)<max_l+2*pad    # accomplish 64 +1 size by adding zeros till finish
            push!(x,0)
    end

    return  x
end


function getSen(vector)
    t=Array{Int}(vector)
    println(permutedims(t))
    for i in t
        if i==1557;print("?\n y =",t[end]+1 );break;end
        if i==0;continue;end
        for (key,value) in word_idx_map
            if value==i; print(key," ");end
        end
    end
end


#Transforms sentence into a list of indices. Pad with zeroes.
function get_tree_rep(r,word_idx_map)
# question
#@show t=r["tree"] #the tree of question
    each_sent=deepcopy(r)# output matrix
    for (j, each_word) in enumerate(each_sent[1:end-1])
        #@show (j, each_word)
            for (l, each_field) in enumerate(each_word)
           # @show (l, each_field)
                if each_field in keys( word_idx_map)
                #@show j,l ;
                    each_sent[j]=Array{Any,1}(each_sent[j])
                     each_sent[j][l] = word_idx_map[each_field]
                elseif each_field == 0
                    continue
                else
                    @show each_field
                end
            end
    end
    return each_sent;
end

function train_dev_test(revs)
    s1,s2,s3=[],[],[]
    t1,t2,t3=[],[],[]
    for rev in revs
    sent =get_text_mat(rev["text"], word_idx_map)
    push!(sent,rev["y"])
    sent_tensor = get_tree_rep(rev["tree"], word_idx_map)

    if rev["split"]==1
            push!(s1,Array{Int}(sent))
            push!(t1,sent_tensor)
    elseif rev["split"]==2
            push!(s2,Array{Int}(sent))
            push!(t2,sent_tensor)
    elseif rev["split"]==3
            push!(s3,Array{Int}(sent))
            push!(t3,sent_tensor)
    end
end

    train = hcat([f1 for f1 in s1]...)
    test =hcat([f1 for f1 in s2]...)
    dev = hcat([f1 for f1 in s3]...)
    train_tensor = t1
    test_tensor = t2
    dev_tensor = t3
    return (train,test,dev),(train_tensor,test_tensor,dev_tensor)
end

dataset,datasetTensor=train_dev_test(revs);

train_seq=vcat([permutedims(vcat(datasetTensor[1][x][1:end-1]...)) for x in 1:size(datasetTensor[1],1)]);
test_seq=vcat([permutedims(vcat(datasetTensor[2][x][1:end-1]...)) for x in 1:size(datasetTensor[2],1)]);

y_train=Array{Int8}([dataset[1][:,x][end] for x in 1:size(dataset[1],2)]);
y_test=Array{Int8}([dataset[2][:,x][end] for x in 1:size(dataset[2],2)]);
y_train=y_train.+1;
y_test=y_test.+1;

#sequence example input
#reshape(W[:, permutedims(hcat(train_seq[1]...))],(300,450,1,1));

#minibatching
dtrn=minibatch(train_seq,y_train,160,shuffle=true);
dtst=minibatch(test_seq,y_test,160);

#minibatch example input
reshape(W[:, permutedims(hcat(first(dtrn)[1]...))],(300,450,1,160));

function guassian(w1,w2,cx,cy;range=[0.01,-0.01])
    return Knet.Param(KnetArray{Float32}((rand(w1,w2,cx,cy).*0.02).- 0.01))
end

struct Chain
    layers; λ1; λ2
    Chain(layers...; λ1=0, λ2=0) = new(layers, λ1, λ2)
end
function (c::Chain)(x)
    x=KnetArray{Float32}(reshape(W[:, vec(hcat(x...))],(300,450,1,160)))
    all=vcat(c.layers[1](x),c.layers[2](x),c.layers[3](x))
    return c.layers[4](all)
    
end
(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)
function (c::Chain)(x,y)
    loss = nll(c(x),y)
    if training() # Only apply regularization during training, only to weights, not biases.
        c.λ1 != 0 && (loss += c.λ1 * sum(sum(abs, l.w) for l in c.layers))
        c.λ2 != 0 && (loss += c.λ2 * sum(sum(abs2,l.w) for l in c.layers))
    end
    return loss
end

# Define a convolutional layer:
struct Conv; w; b; f; p;E; end
function (c::Conv)(x)
    conv=conv4(c.w, dropout(x,c.p))
    r=reshape(c.f.(pool(conv,window=(1,size(conv)[2])).+c.b),(size(conv)[3],size(conv)[4]))
    return r
end
#with Xavier
Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0,E=W) = Conv(param(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop,E)

#with Guassian
#Conv(w1::Int,w2::Int,cx::Int,cy::Int,f=relu;pdrop=0,E=W) = Conv(guassian(w1,w2,cx,cy), param0(1,1,cy,1), f, pdrop,E)


#define dense layer :
struct Dense; w; b; f; p; end
function (d::Dense)(x)
    d.f.(d.w * mat(dropout(x,d.p)) .+ d.b)
end

Dense(i::Int,o::Int,f=Knet.identity;pdrop=0) = Dense(param(o,i), param0(o), f, pdrop)


function trainresults(file,model; o...)
        println("lr =",lr_decay," \t n_epochs= ",n_epochs)
    if (print("Train from scratch? "); readline()[1]=='y')
        takeevery(n,itr) = (x for (i,x) in enumerate(itr) if i % n == 1)
        r = ((model(dtrn), model(dtst), zeroone(model,dtrn), zeroone(model,dtst))
             for x in takeevery(length(dtrn), progress(adadelta(model,repeat(dtrn,n_epochs),lr=lr_decay))))
        r = reshape(collect(Float32,flatten(r)),(4,:))
        Knet.save(file,"results",r)
        Knet.gc() # To save gpu memory
    else
        if isfile(file);r=Knet.load(file,"results");else;println("there is no file such this");return;end
    end
    println(minimum(r,dims=2))
    return r
end

d=300
DCNN=Chain(Conv(d,3,1,100)
,Conv(d,4,1,100)
,Conv(d,5,1,100)
,Dense(300,6,pdrop=0.5),λ1=4f-6)
summary.(l.w for l in DCNN.layers)

n_epochs=17;
lr_decay = 0.95
results=trainresults("models/dcnn13_3.jld2", DCNN);
